{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f740f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created by Alexander Bolejack & Austin Roddy\n",
    "# Illinois Institute of Technology\n",
    "# Intllgnc Txt Analys Knwldg Mgm (CS-481-01), Spring 2022\n",
    "# Programming Assignment 2 - Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f81af55-9a47-4e68-bf37-dd69c1e0d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inport the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords as sw\n",
    "import re\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8200dd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dowload corpori for nltk (used later)\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d879f01-5562-42a3-85bb-135d70a0f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data, and drop the unnecessary columns, remove non-alphabetical characters(first pass)\n",
    "full_dataset_df = pd.read_csv (\"dataset.csv\")\n",
    "full_dataset_df.drop('app_id', inplace=True, axis=1)\n",
    "full_dataset_df.drop('app_name', inplace=True, axis=1)\n",
    "full_dataset_df.drop('review_votes', inplace=True, axis=1)\n",
    "full_dataset_df.rename(columns={'review_text': 'text', 'review_score': 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d035f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate messages in entries in the dataset\n",
    "duplicates = full_dataset_df.duplicated(subset='text', keep='first')\n",
    "full_dataset_df = full_dataset_df.drop(full_dataset_df[duplicates].index)\n",
    "\n",
    "\n",
    "# I wouldnt trust this function HAHAHAHA it deletes 1.6 million rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c7d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the labels if -1 switch to 0\n",
    "full_dataset_df['label'] = full_dataset_df['label'].replace(-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "705caa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the dataframe\n",
    "randomized_full_dataset_df = full_dataset_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a550077-77e7-4add-a737-4cae17ad13fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (90% training, 10% testing) (and randomize... again...? *shrug*)\n",
    "training_dataset_df = randomized_full_dataset_df.sample(frac=0.9,random_state=200)\n",
    "testing_dataset_df = randomized_full_dataset_df.drop(training_dataset_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0874499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create both pos and neg versions of training and testing sets (4 sets)\n",
    "training_pos_dataset_df = training_dataset_df[training_dataset_df['label'] == 1]\n",
    "training_neg_dataset_df = training_dataset_df[training_dataset_df['label'] == 0]\n",
    "testing_pos_dataset_df = testing_dataset_df[testing_dataset_df['label'] == 1]\n",
    "testing_neg_dataset_df = testing_dataset_df[testing_dataset_df['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb787a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp/ipykernel_27280/4058182508.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_pos_dataset_df['text'] = training_pos_dataset_df['text'].astype(str)\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp/ipykernel_27280/4058182508.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_neg_dataset_df['text'] = training_neg_dataset_df['text'].astype(str)\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp/ipykernel_27280/4058182508.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testing_pos_dataset_df['text'] = testing_pos_dataset_df['text'].astype(str)\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp/ipykernel_27280/4058182508.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testing_neg_dataset_df['text'] = testing_neg_dataset_df['text'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Convert ALL text rows to string, if not already string (this was causing issues which is why im now converting to string)\n",
    "# Ignore the output warnings, or better yet, disable them. >:) (this is a bit of a hack, but it works)\n",
    "training_pos_dataset_df['text'] = training_pos_dataset_df['text'].astype(str)\n",
    "training_neg_dataset_df['text'] = training_neg_dataset_df['text'].astype(str)\n",
    "testing_pos_dataset_df['text'] = testing_pos_dataset_df['text'].astype(str)\n",
    "testing_neg_dataset_df['text'] = testing_neg_dataset_df['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "424ef08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\anaconda3\\envs\\VSCode\\lib\\site-packages\\pandas\\core\\indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "# Lowercase ALL sets\n",
    "# Ignore the output warnings, or better yet, disable them. >:) (this is a bit of a hack, but it works)\n",
    "training_pos_dataset_df.loc[:,'text'] = training_pos_dataset_df.loc[:,'text'].apply(lambda x: x.lower())\n",
    "training_neg_dataset_df.loc[:,'text'] = training_neg_dataset_df.loc[:,'text'].apply(lambda x: x.lower())\n",
    "testing_pos_dataset_df.loc[:,'text'] = testing_pos_dataset_df.loc[:,'text'].apply(lambda x: x.lower())\n",
    "testing_neg_dataset_df.loc[:,'text'] = testing_neg_dataset_df.loc[:,'text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cddb18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe rows to lists\n",
    "training_pos_dataset_list = training_pos_dataset_df['text'].tolist()\n",
    "training_neg_dataset_list = training_neg_dataset_df['text'].tolist()\n",
    "testing_pos_dataset_list = testing_pos_dataset_df['text'].tolist()\n",
    "testing_neg_dataset_list = testing_neg_dataset_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8781e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all non-alphabetical characters\n",
    "training_pos_dataset_list_alpha_cleaned = []\n",
    "training_neg_dataset_list_alpha_cleaned = []\n",
    "testing_pos_dataset_list_alpha_cleaned = []\n",
    "testing_neg_dataset_list_alpha_cleaned = []\n",
    "\n",
    "for i in range(len(training_pos_dataset_list)):\n",
    "    training_pos_dataset_list_alpha_cleaned.append(''.join([ch for ch in training_pos_dataset_list[i] if ch.isalpha() or ch == ' ']))\n",
    "\n",
    "for i in range(len(training_neg_dataset_list)):\n",
    "    training_neg_dataset_list_alpha_cleaned.append(''.join([ch for ch in training_neg_dataset_list[i] if ch.isalpha() or ch == ' ']))\n",
    "\n",
    "for i in range(len(testing_pos_dataset_list)):\n",
    "    testing_pos_dataset_list_alpha_cleaned.append(''.join([ch for ch in testing_pos_dataset_list[i] if ch.isalpha() or ch == ' ']))\n",
    "\n",
    "for i in range(len(testing_neg_dataset_list)):\n",
    "    testing_neg_dataset_list_alpha_cleaned.append(''.join([ch for ch in testing_neg_dataset_list[i] if ch.isalpha() or ch == ' ']))\n",
    "\n",
    "# wow i cant believe that worked?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5d9fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each list\n",
    "training_pos_dataset_list_alpha_cleaned_tokenized = []\n",
    "training_neg_dataset_list_alpha_cleaned_tokenized = []\n",
    "testing_pos_dataset_list_alpha_cleaned_tokenized = []\n",
    "testing_neg_dataset_list_alpha_cleaned_tokenized = []\n",
    "\n",
    "for i in range(len(training_pos_dataset_list_alpha_cleaned)):\n",
    "    training_pos_dataset_list_alpha_cleaned_tokenized += training_pos_dataset_list_alpha_cleaned[i].split()\n",
    "\n",
    "for i in range(len(training_neg_dataset_list_alpha_cleaned)):\n",
    "    training_neg_dataset_list_alpha_cleaned_tokenized += training_neg_dataset_list_alpha_cleaned[i].split()\n",
    "\n",
    "for i in range(len(testing_pos_dataset_list_alpha_cleaned)):\n",
    "    testing_pos_dataset_list_alpha_cleaned_tokenized += [testing_pos_dataset_list_alpha_cleaned[i].split()]\n",
    "\n",
    "for i in range(len(testing_neg_dataset_list_alpha_cleaned)):\n",
    "    testing_neg_dataset_list_alpha_cleaned_tokenized += [testing_neg_dataset_list_alpha_cleaned[i].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3af2da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "training_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed = []\n",
    "training_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed = []\n",
    "testing_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed = []\n",
    "testing_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed = []\n",
    "stop_words = set(sw.words('english'))\n",
    "\n",
    "\n",
    "for i in training_pos_dataset_list_alpha_cleaned_tokenized:\n",
    "    if i not in stop_words:\n",
    "        training_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed.append(i)\n",
    "\n",
    "for i in training_neg_dataset_list_alpha_cleaned_tokenized:\n",
    "    if i not in stop_words:\n",
    "        training_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed.append(i)\n",
    "\n",
    "        \n",
    "\n",
    "for i in testing_pos_dataset_list_alpha_cleaned_tokenized:\n",
    "    words = []\n",
    "    for w in i:\n",
    "        if w not in stop_words:\n",
    "            words.append(w)\n",
    "    testing_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed += [words]\n",
    "\n",
    "for i in testing_neg_dataset_list_alpha_cleaned_tokenized:\n",
    "    words = []\n",
    "    for w in i:\n",
    "        if w not in stop_words:\n",
    "            words.append(w)\n",
    "    testing_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed += [words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17caf949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmed each set:\n",
    "ps = PorterStemmer()\n",
    "\n",
    "training_pos_dataset_list_alpha_cleaned_stemmed = []\n",
    "training_neg_dataset_list_alpha_cleaned_stemmed = []\n",
    "\n",
    "\n",
    "for i in range(len(training_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed)):\n",
    "    training_pos_dataset_list_alpha_cleaned_stemmed.append(ps.stem(training_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed[i]))\n",
    "\n",
    "for i in range(len(training_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed)):\n",
    "    training_neg_dataset_list_alpha_cleaned_stemmed.append(ps.stem(training_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d5298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem the test. Seperate from training in case we load the model in but want to run the test data\n",
    "testing_pos_dataset_list_alpha_cleaned_stemmed = []\n",
    "testing_neg_dataset_list_alpha_cleaned_stemmed = []\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range(len(testing_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed)):\n",
    "    words = []\n",
    "    for w in range(len(testing_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed[i])):\n",
    "        words.append(ps.stem(testing_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed[i][w]))\n",
    "    testing_pos_dataset_list_alpha_cleaned_stemmed += [words]\n",
    "\n",
    "\n",
    "for i in range(len(testing_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed)):\n",
    "    words = []\n",
    "    for w in range(len(testing_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed[i])):\n",
    "        words.append(ps.stem(testing_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed[i][w]))\n",
    "    testing_neg_dataset_list_alpha_cleaned_stemmed += [words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8f1d6",
   "metadata": {},
   "source": [
    "### With clean data now, we can make our Bayes Model\n",
    "##### For this, we can get work counts over the total number of words for each class\n",
    "##### Using these values, as well as the probability of the class, we can make our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0813f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probability of class\n",
    "probability_pos = len(training_pos_dataset_df)/ len(training_dataset_df)\n",
    "probability_neg = len(training_neg_dataset_df)/ len(training_dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d69a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get total count of words for each class\n",
    "pos_denom = len(training_pos_dataset_list_alpha_cleaned_stemmed)\n",
    "neg_denom = len(training_neg_dataset_list_alpha_cleaned_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdefacd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get counts of the words for each class\n",
    "pos_nom = FreqDist(training_pos_dataset_list_alpha_cleaned_stemmed)\n",
    "neg_nom = FreqDist(training_neg_dataset_list_alpha_cleaned_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aebd8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets get information about smoothing. Simple laplace +1\n",
    "pos_vocab_size = len(pos_nom)\n",
    "neg_vocab_size = len(neg_nom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42a5b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(probability, denom, vocab_size, nom, name):\n",
    "    with open(name, 'w') as f:\n",
    "        f.write(str(probability) + '\\n')\n",
    "        f.write(str(denom) + '\\n')\n",
    "        f.write(str(vocab_size) + '\\n\\n')\n",
    "        for i in list(nom.items()):\n",
    "            f.write(str(i[0]) +' ' + str(i[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9edde397",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(probability_pos, pos_denom, pos_vocab_size, pos_nom, 'pos_prob.txt')\n",
    "save_model(probability_neg, neg_denom, neg_vocab_size, neg_nom, 'neg_prob.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b09eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file):\n",
    "    model = open(file, 'r')\n",
    "    Lines = model.readlines()\n",
    "    count = 0\n",
    "    wourd_count_dict = {}\n",
    "    for line in Lines:\n",
    "        if count == 0:\n",
    "            class_probability = line\n",
    "        elif count == 1:\n",
    "            denominator = line\n",
    "        elif count == 2:\n",
    "            vocab_size = line\n",
    "        else:\n",
    "            line_split = line.split()\n",
    "            if len(line_split) == 2:\n",
    "                wourd_count_dict[line_split[0]] = int(line_split[1])\n",
    "        count = count + 1\n",
    "\n",
    "    return float(class_probability), int(denominator), int(vocab_size), wourd_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a93a28e",
   "metadata": {},
   "source": [
    "### Lets have an inference function\n",
    "\n",
    "##### Takes in a list of strings, that has aready been tokenized etc,\n",
    "##### Returns the predicted class, 1 or -1\n",
    "##### Optional print value, enabled by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5006b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sentence_precoessed,prob_pos, pos_denom, pos_vocab, pos_nom,prob_neg, neg_denom, neg_vocab, neg_nom, p_print=True):\n",
    "    pos_value = 1\n",
    "    neg_value = 1\n",
    "    for word in sentence_precoessed:\n",
    "        if word in pos_nom:\n",
    "            pos_value = pos_value * ( (pos_nom[word]+1) / (pos_denom+pos_vocab) )\n",
    "        if word in neg_nom:  \n",
    "            neg_value = neg_value * ( (neg_nom[word]+1) / (neg_denom+neg_vocab) )\n",
    "\n",
    "    pos_value = pos_value * prob_pos\n",
    "    neg_value = neg_value * prob_neg\n",
    "\n",
    "    if p_print:\n",
    "        print('Positive review prediction: ' + str(pos_value))\n",
    "        print('Negative review prediction: ' + str(neg_value))\n",
    "\n",
    "    if pos_value >= neg_value:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c9d688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    new_sentence = []\n",
    "    new_sentence.append(''.join([ch for ch in sentence if ch.isalpha() or ch == ' ']))\n",
    "    new_sentence = new_sentence[0].split()\n",
    "\n",
    "    stop_words = set(sw.words('english'))\n",
    "    sentence_no_stop_words = []\n",
    "    for i in new_sentence:\n",
    "        if i not in stop_words:\n",
    "            sentence_no_stop_words.append(i)\n",
    "\n",
    "    sentence_stemmed = []\n",
    "\n",
    "    for i in range(len(sentence_no_stop_words)):\n",
    "        sentence_stemmed.append(ps.stem(sentence_no_stop_words[i]))\n",
    "\n",
    "    return sentence_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d892fc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['peopl', 'fall', 'braindead', 'mess', 'minigam', 'fill', 'obnoxi', 'cutscen', 'beyond', 'medo', 'favor', 'go', 'watch', 'yakuza', 'movi', 'instead', 'like', 'set', 'guarante', 'theyr', 'funnier']\n",
      "Positive review prediction: 2.0686892714848466e-82\n",
      "Negative review prediction: 1.4591353478242677e-74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#From a sentence to input\n",
    "sentence = \"How people can fall for this braindead mess of minigames filled with obnoxious cutscenes is beyond me.Do yourself a favor and go watch some yakuza movies instead if you like this setting, I guarantee they're funnier.\"\n",
    "sentence_normalized = normalize_sentence(sentence)\n",
    "print(sentence_normalized)\n",
    "class_prob_pos, denom_pos, vocab_pos, wourd_dict_pos = load_model('pos_prob.txt')\n",
    "class_prob_neg, denom_neg, vocab_neg, wourd_dict_neg = load_model('neg_prob.txt')\n",
    "prediction = inference(sentence_normalized, class_prob_pos, denom_pos, vocab_pos, wourd_dict_pos, class_prob_neg, denom_neg, vocab_neg, wourd_dict_neg)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc64032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
