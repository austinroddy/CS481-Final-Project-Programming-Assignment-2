{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f740f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created by Alexander Bolejack & Austin Roddy\n",
    "# Illinois Institute of Technology\n",
    "# Intllgnc Txt Analys Knwldg Mgm (CS-481-01), Spring 2022\n",
    "# Programming Assignment 2 - Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f81af55-9a47-4e68-bf37-dd69c1e0d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inport the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords as sw\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8200dd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\austi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dowload corpori for nltk (used later)\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d879f01-5562-42a3-85bb-135d70a0f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data, and drop the unnecessary columns, remove non-alphabetical characters(first pass)\n",
    "full_dataset_df = pd.read_csv (\"dataset.csv\")\n",
    "full_dataset_df.drop('app_id', inplace=True, axis=1)\n",
    "full_dataset_df.drop('app_name', inplace=True, axis=1)\n",
    "full_dataset_df.drop('review_votes', inplace=True, axis=1)\n",
    "full_dataset_df.rename(columns={'review_text': 'text', 'review_score': 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d035f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate messages in entries in the dataset\n",
    "duplicates = full_dataset_df.duplicated(subset='text', keep='first')\n",
    "full_dataset_df = full_dataset_df.drop(full_dataset_df[duplicates].index)\n",
    "\n",
    "\n",
    "# I wouldnt trust this function HAHAHAHA it deletes 1.6 million rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c7d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the labels if -1 switch to 0\n",
    "full_dataset_df['label'] = full_dataset_df['label'].replace(-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "705caa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the dataframe\n",
    "randomized_full_dataset_df = full_dataset_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a550077-77e7-4add-a737-4cae17ad13fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (90% training, 10% testing) (and randomize... again...? *shrug*)\n",
    "training_dataset_df = randomized_full_dataset_df.sample(frac=0.9,random_state=200)\n",
    "testing_dataset_df = randomized_full_dataset_df.drop(training_dataset_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0874499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create both pos and neg versions of training and testing sets (4 sets)\n",
    "training_pos_dataset_df = training_dataset_df[training_dataset_df['label'] == 1]\n",
    "training_neg_dataset_df = training_dataset_df[training_dataset_df['label'] == 0]\n",
    "testing_pos_dataset_df = testing_dataset_df[testing_dataset_df['label'] == 1]\n",
    "testing_neg_dataset_df = testing_dataset_df[testing_dataset_df['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb787a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-3fe0b7d84b32>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_pos_dataset_df['text'] = training_pos_dataset_df['text'].astype(str)\n",
      "<ipython-input-10-3fe0b7d84b32>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_neg_dataset_df['text'] = training_neg_dataset_df['text'].astype(str)\n",
      "<ipython-input-10-3fe0b7d84b32>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testing_pos_dataset_df['text'] = testing_pos_dataset_df['text'].astype(str)\n",
      "<ipython-input-10-3fe0b7d84b32>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testing_neg_dataset_df['text'] = testing_neg_dataset_df['text'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Convert ALL text rows to string, if not already string (this was causing issues which is why im now converting to string)\n",
    "# Ignore the output warnings, or better yet, disable them. >:) (this is a bit of a hack, but it works)\n",
    "training_pos_dataset_df['text'] = training_pos_dataset_df['text'].astype(str)\n",
    "training_neg_dataset_df['text'] = training_neg_dataset_df['text'].astype(str)\n",
    "testing_pos_dataset_df['text'] = testing_pos_dataset_df['text'].astype(str)\n",
    "testing_neg_dataset_df['text'] = testing_neg_dataset_df['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "424ef08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "# Lowercase ALL sets\n",
    "# Ignore the output warnings, or better yet, disable them. >:) (this is a bit of a hack, but it works)\n",
    "training_pos_dataset_df.loc[:,'text'] = training_pos_dataset_df.loc[:,'text'].apply(lambda x: x.lower())\n",
    "training_neg_dataset_df.loc[:,'text'] = training_neg_dataset_df.loc[:,'text'].apply(lambda x: x.lower())\n",
    "testing_pos_dataset_df.loc[:,'text'] = testing_pos_dataset_df.loc[:,'text'].apply(lambda x: x.lower())\n",
    "testing_neg_dataset_df.loc[:,'text'] = testing_neg_dataset_df.loc[:,'text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cddb18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe rows to lists\n",
    "training_pos_dataset_list = training_pos_dataset_df['text'].tolist()\n",
    "training_neg_dataset_list = training_neg_dataset_df['text'].tolist()\n",
    "testing_pos_dataset_list = testing_pos_dataset_df['text'].tolist()\n",
    "testing_neg_dataset_list = testing_neg_dataset_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8781e34",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d75624bf2fed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_pos_dataset_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtraining_pos_dataset_list_alpha_cleaned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtraining_pos_dataset_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_neg_dataset_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Remove all non-alphabetical characters\n",
    "training_pos_dataset_list_alpha_cleaned = []\n",
    "training_neg_dataset_list_alpha_cleaned = []\n",
    "testing_pos_dataset_list_alpha_cleaned = []\n",
    "testing_neg_dataset_list_alpha_cleaned = []\n",
    "\n",
    "for i in range(len(training_pos_dataset_list)):\n",
    "    training_pos_dataset_list_alpha_cleaned.append(''.join([ch for ch in training_pos_dataset_list[i] if ch.isalpha() or ch == ' ']))\n",
    "\n",
    "for i in range(len(training_neg_dataset_list)):\n",
    "    training_neg_dataset_list_alpha_cleaned.append(''.join([ch for ch in training_neg_dataset_list[i] if ch.isalpha() or ch == ' ']))\n",
    "\n",
    "for i in range(len(testing_pos_dataset_list)):\n",
    "    testing_pos_dataset_list_alpha_cleaned.append(''.join([ch for ch in testing_pos_dataset_list[i] if ch.isalpha() or ch == ' ']))\n",
    "\n",
    "for i in range(len(testing_neg_dataset_list)):\n",
    "    testing_neg_dataset_list_alpha_cleaned.append(''.join([ch for ch in testing_neg_dataset_list[i] if ch.isalpha() or ch == ' ']))\n",
    "\n",
    "# wow i cant believe that worked?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d9fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each list\n",
    "training_pos_dataset_list_alpha_cleaned_tokenized = []\n",
    "training_neg_dataset_list_alpha_cleaned_tokenized = []\n",
    "testing_pos_dataset_list_alpha_cleaned_tokenized = []\n",
    "testing_neg_dataset_list_alpha_cleaned_tokenized = []\n",
    "\n",
    "for i in range(len(training_pos_dataset_list_alpha_cleaned)):\n",
    "    training_pos_dataset_list_alpha_cleaned_tokenized += training_pos_dataset_list_alpha_cleaned[i].split()\n",
    "\n",
    "for i in range(len(training_neg_dataset_list_alpha_cleaned)):\n",
    "    training_neg_dataset_list_alpha_cleaned_tokenized += training_neg_dataset_list_alpha_cleaned[i].split()\n",
    "\n",
    "# for i in range(len(testing_pos_dataset_list_alpha_cleaned)):\n",
    "#     testing_pos_dataset_list_alpha_cleaned_tokenized += testing_pos_dataset_list_alpha_cleaned[i].split()\n",
    "\n",
    "# for i in range(len(testing_neg_dataset_list_alpha_cleaned)):\n",
    "#     testing_neg_dataset_list_alpha_cleaned_tokenized += testing_neg_dataset_list_alpha_cleaned[i].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af2da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "training_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed = []\n",
    "training_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed = []\n",
    "testing_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed = []\n",
    "testing_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed = []\n",
    "stop_words = set(sw.words('english'))\n",
    "\n",
    "\n",
    "for i in training_pos_dataset_list_alpha_cleaned_tokenized:\n",
    "    if i not in stop_words:\n",
    "        training_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed.append(i)\n",
    "\n",
    "for i in training_neg_dataset_list_alpha_cleaned_tokenized:\n",
    "    if i not in stop_words:\n",
    "        training_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed.append(i)\n",
    "\n",
    "# for i in testing_pos_dataset_list_alpha_cleaned_tokenized:\n",
    "#     if i not in stop_words:\n",
    "#         testing_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed.append(i)\n",
    "\n",
    "# for i in testing_neg_dataset_list_alpha_cleaned_tokenized:\n",
    "#     if i not in stop_words:\n",
    "#         testing_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17caf949",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lt/mn6whq3x1l106f5w78wq5x780000gn/T/ipykernel_11138/836119143.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtraining_pos_dataset_list_alpha_cleaned_stemmed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word, to_lowercase)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mto_lowercase\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLTK_EXTENSIONS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Stemmed each set:\n",
    "ps = PorterStemmer()\n",
    "\n",
    "training_pos_dataset_list_alpha_cleaned_stemmed = []\n",
    "training_neg_dataset_list_alpha_cleaned_stemmed = []\n",
    "testing_pos_dataset_list_alpha_cleaned_stemmed = []\n",
    "testing_neg_dataset_list_alpha_cleaned_stemmed = []\n",
    "\n",
    "for i in range(len(training_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed)):\n",
    "    training_pos_dataset_list_alpha_cleaned_stemmed.append(ps.stem(training_pos_dataset_list_alpha_cleaned_tokenized_stopwords_removed[i]))\n",
    "\n",
    "for i in range(len(training_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed)):\n",
    "    training_neg_dataset_list_alpha_cleaned_stemmed.append(ps.stem(training_neg_dataset_list_alpha_cleaned_tokenized_stopwords_removed[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
